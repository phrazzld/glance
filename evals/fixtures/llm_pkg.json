{
  "directory": "glance/llm",
  "files": {
    "prompt.go": "// Package llm provides abstractions and implementations for interacting with\n// Large Language Model APIs in the glance application.\npackage llm\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"strings\"\n\t\"text/template\"\n)\n\n// PromptData holds the content used to generate prompts for LLM requests.\n// It contains information about the directory structure, content, and summaries\n// that will be filled into the prompt template.\ntype PromptData struct {\n\t// Directory is the path to the directory being processed\n\tDirectory string\n\n\t// SubGlances contains the compiled contents of subdirectory glance.md files\n\tSubGlances string\n\n\t// FileContents contains the formatted contents of files in the directory\n\tFileContents string\n}\n\n// DefaultTemplate returns the default prompt template used for generating directory summaries.\n// This template is used when no custom template is provided.\nfunc DefaultTemplate() string {\n\treturn `you are an expert code reviewer and technical writer.\ngenerate a concise, factual technical summary for this directory.\nUse only what is present in the provided source snippets (directory summaries + file contents + explicit structure).\n\nHard constraints:\n- do NOT describe CLI flags, command-line options, defaults, runtime modes, side effects, or performance characteristics unless they are explicitly defined in the provided source.\n- do NOT speculate about behavior, configuration, environment variables, dependencies, or architecture details not evidenced by the provided content.\n- do NOT provide recommendations, next steps, or hypothetical refactors.\n- if a claim cannot be verified from the provided source, omit it rather than infer.\n- do NOT mention files or directories that are not listed in the provided input.\n\nOutput format:\n## Purpose\nOne short paragraph (max 5 sentences) describing the directory-level intent.\n\n## Key Roles\n- list major files and their responsibilities\n- if no obvious key roles are found, state \"No dominant file roles detected.\"\n\n## Dependencies and Caveats\n- list important dependencies and notable caveats grounded in the provided source\n- max 8 bullets\n\nKeep this output under 400 words.\n\nrespond with ONLY the sections above, in the exact order shown.\n\ndirectory: {{.Directory}}\n\nsubdirectory summaries:\n{{.SubGlances}}\n\nlocal file contents:\n{{.FileContents}}\n`\n}\n\n// GeneratePrompt generates a prompt by filling the template with the provided data.\n//\n// Parameters:\n//   - data: The PromptData to use for template variables\n//   - templateStr: The template string to use\n//\n// Returns:\n//   - The generated prompt as a string\n//   - An error if template parsing or execution fails\nfunc GeneratePrompt(data *PromptData, templateStr string) (string, error) {\n\t// Parse the template\n\ttmpl, err := template.New(\"prompt\").Parse(templateStr)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"failed to parse prompt template: %w\", err)\n\t}\n\n\t// Execute the template with the provided data\n\tvar rendered bytes.Buffer\n\tif err = tmpl.Execute(&rendered, data); err != nil {\n\t\treturn \"\", fmt.Errorf(\"failed to execute prompt template: %w\", err)\n\t}\n\n\treturn rendered.String(), nil\n}\n\n// FormatFileContents formats a map of filenames to content for inclusion in a prompt.\n// The format used is \"=== file: {filename} ===\\n{content}\\n\\n\".\n//\n// Parameters:\n//   - fileMap: A map of filenames to their content\n//\n// Returns:\n//   - A formatted string containing all file contents\nfunc FormatFileContents(fileMap map[string]string) string {\n\tvar builder strings.Builder\n\n\tfor filename, content := range fileMap {\n\t\tbuilder.WriteString(fmt.Sprintf(\"=== file: %s ===\\n%s\\n\\n\", filename, content))\n\t}\n\n\treturn builder.String()\n}\n\n// BuildPromptData creates a PromptData structure with the provided information.\n// It formats the file contents using FormatFileContents.\n//\n// Parameters:\n//   - dir: The directory path\n//   - subGlances: Compiled content from subdirectory glance.md files\n//   - fileMap: A map of filenames to their content\n//\n// Returns:\n//   - A populated PromptData structure\nfunc BuildPromptData(dir string, subGlances string, fileMap map[string]string) *PromptData {\n\treturn &PromptData{\n\t\tDirectory:    dir,\n\t\tSubGlances:   subGlances,\n\t\tFileContents: FormatFileContents(fileMap),\n\t}\n}\n",
    "client.go": "// Package llm provides abstractions and implementations for interacting with\n// Large Language Model APIs in the glance application.\npackage llm\n\nimport (\n\t\"context\"\n\t\"errors\" // For errors.Is\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/sirupsen/logrus\"\n\t\"google.golang.org/genai\"\n\n\tcustomerrors \"glance/errors\" // Application's custom error package\n)\n\n// Client defines the interface for interacting with LLM services.\n// This interface abstracts the underlying LLM provider, making it easier\n// to switch providers or mock in tests.\ntype Client interface {\n\t// Generate takes a prompt and returns the generated text.\n\t// It handles all API interaction details and returns only the final result.\n\tGenerate(ctx context.Context, prompt string) (string, error)\n\n\t// GenerateStream takes a prompt and returns a channel of generated text chunks.\n\t// It enables streaming responses from the LLM API for incremental processing.\n\t// Consumers must read from the channel until it's closed to avoid resource leaks.\n\tGenerateStream(ctx context.Context, prompt string) (<-chan StreamChunk, error)\n\n\t// CountTokens counts the number of tokens in the provided prompt.\n\t// This is useful for understanding API usage and costs.\n\tCountTokens(ctx context.Context, prompt string) (int, error)\n\n\t// Close releases any resources used by the client.\n\t// It should be called when the client is no longer needed.\n\tClose()\n}\n\n// StreamChunk represents a piece of content from a streaming LLM response.\n// It contains either content text or an error encountered during streaming.\ntype StreamChunk struct {\n\t// Text contains the text content of this chunk, if any\n\tText string\n\n\t// Error contains any error encountered during streaming\n\tError error\n\n\t// Done indicates that this is the final chunk of the stream\n\tDone bool\n",
    "service.go": "// Package llm provides abstractions and implementations for interacting with\n// Large Language Model APIs in the glance application.\npackage llm\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/sirupsen/logrus\"\n)\n\n// Service provides high-level LLM operations for the Glance application.\n// It encapsulates a Client and provides application-specific functionality\n// for generating directory summaries.\ntype Service struct {\n\tclient         Client\n\tmaxRetries     int\n\tmodelName      string\n\tpromptTemplate string\n}\n\n// ServiceConfig contains configuration for creating a new Service.\n// This simplifies the pattern for service creation while maintaining flexibility.\ntype ServiceConfig struct {\n\t// MaxRetries is the number of times to retry failed LLM operations\n\tMaxRetries int\n\n\t// ModelName is the name of the LLM model to use\n\tModelName string\n\n\t// PromptTemplate is the template string to use for generating prompts\n\tPromptTemplate string\n}\n\n// DefaultServiceConfig returns a ServiceConfig with sensible defaults.\n// It uses the same default model as the client configuration.\nfunc DefaultServiceConfig() ServiceConfig {\n\treturn ServiceConfig{\n\t\tMaxRetries:     3,\n\t\tModelName:      \"gemini-3-flash-preview\", // Make sure this matches the client default\n\t\tPromptTemplate: \"\",\n\t}\n}\n\n// WithServiceMaxRetries configures the maximum number of retries for the service.\nfunc WithServiceMaxRetries(maxRetries int) func(*ServiceConfig) {\n\treturn func(c *ServiceConfig) {\n\t\tc.MaxRetries = maxRetries\n\t}\n}\n\n// WithServiceModelName configures the model name for the service.\nfunc WithServiceModelName(modelName string) func(*ServiceConfig) {\n\treturn func(c *ServiceConfig) {\n\t\tc.ModelName = modelName\n\t}\n}\n\n// WithPromptTemplate configures the prompt template for the service.\nfunc WithPromptTemplate(template string) func(*ServiceConfig) {\n\treturn func(c *ServiceConfig) {\n\t\tc.PromptTemplate = template\n\t}\n}\n\n// NewService creates a new LLM Service with the specified client and options.\n//\n// Parameters:\n//   - client: The LLM client to use for API interactions\n//   - options: Optional functional options to configure the service\n//\n// Returns:\n//   - A new Service instance\n//   - An error if service creation fails\nfunc NewService(client Client, options ...func(*ServiceConfig)) (*Service, error) {\n\tif client == nil {\n\t\treturn nil, fmt.Errorf(\"client cannot be nil\")\n\t}\n\n\t// Start with default config\n\tconfig := DefaultServiceConfig()\n\n\t// Apply any provided options\n\tfor _, option := range options {\n\t\toption(&config)\n\t}\n\n\treturn &Service{\n\t\tclient:         client,\n\t\tmaxRetries:     config.MaxRetries,\n\t\tmodelName:      config.ModelName,\n\t\tpromptTemplate: config.PromptTemplate,\n\t}, nil\n}\n\n",
    "fallback_client.go": "// Package llm provides abstractions and implementations for interacting with\n// Large Language Model APIs in the glance application.\npackage llm\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/sirupsen/logrus\"\n\n\tcustomerrors \"glance/errors\"\n)\n\nconst (\n\tdefaultFallbackBackoff    = 250 * time.Millisecond\n\tdefaultFallbackMaxBackoff = 4 * time.Second\n)\n\n// FallbackTier defines a model/provider tier in a failover chain.\ntype FallbackTier struct {\n\tName   string\n\tClient Client\n}\n\n// FallbackClient tries generation with retries on each tier, then falls back\n// to the next tier when a tier is exhausted.\ntype FallbackClient struct {\n\ttiers          []FallbackTier\n\tretriesPerTier int\n\tbaseBackoff    time.Duration\n\tmaxBackoff     time.Duration\n}\n\n// NewFallbackClient creates a fallback client with sensible backoff defaults.\nfunc NewFallbackClient(tiers []FallbackTier, retriesPerTier int) (Client, error) {\n\treturn NewFallbackClientWithBackoff(\n\t\ttiers,\n\t\tretriesPerTier,\n\t\tdefaultFallbackBackoff,\n\t\tdefaultFallbackMaxBackoff,\n\t)\n}\n\n// NewFallbackClientWithBackoff creates a fallback client with explicit backoff settings.\nfunc NewFallbackClientWithBackoff(\n\ttiers []FallbackTier,\n\tretriesPerTier int,\n\tbaseBackoff time.Duration,\n\tmaxBackoff time.Duration,\n) (Client, error) {\n\tif len(tiers) == 0 {\n\t\treturn nil, customerrors.NewValidationError(\"at least one fallback tier is required\", nil).\n\t\t\tWithCode(\"LLM-001\")\n\t}\n\tif retriesPerTier < 0 {\n\t\treturn nil, customerrors.NewValidationError(\"retries per tier cannot be negative\", nil).\n\t\t\tWithCode(\"LLM-002\")\n\t}\n"
  },
  "subGlances": []
}
